import os
from types import SimpleNamespace
from unittest.mock import MagicMock, patch

import torch
import pytest
from omni_npu.platform import NPUPlatform, ConfigUpdater
from tests.unit.platform.utils import create_vllm_config


class TestNPUPlatform:
    def setup_method(self):
        self.vllm_cfg = create_vllm_config()

    def test_platform_attributes(self):
        """Test platform class attributes.
        
        Verifies that NPUPlatform has the correct static attributes
        for device name, type, dispatch key, and backend configuration.
        """
        assert NPUPlatform.device_name == "npu"
        assert NPUPlatform.device_type == "npu"
        assert NPUPlatform.dispatch_key == "PrivateUse1"
        assert NPUPlatform.ray_device_key == "NPU"
        assert NPUPlatform.dist_backend == "hccl"
        assert NPUPlatform.device_control_env_var == "ASCEND_RT_VISIBLE_DEVICES"

    def test_platform_init(self):
        """Test platform initialization.
        
        Verifies that NPUPlatform can be instantiated correctly.
        """
        platform = NPUPlatform()
        assert isinstance(platform, NPUPlatform)

    def test_torch_npu_proxy_methods(self, monkeypatch):
        """Test all methods that directly proxy to torch.npu (consolidated test).
        
        Verifies that all proxy methods correctly delegate to torch.npu
        and return the expected values.
        """
        # Mock torch.npu methods
        mock_set_device = MagicMock()
        mock_get_device_name = MagicMock(return_value="Ascend910")
        mock_device_count = MagicMock(return_value=8)
        mock_mem_get_info = MagicMock(return_value=(1000, 2000))
        mock_reset_peak = MagicMock()
        mock_max_memory = MagicMock(return_value=500.0)

        monkeypatch.setattr("torch.npu.set_device", mock_set_device)
        monkeypatch.setattr("torch.npu.get_device_name", mock_get_device_name)
        monkeypatch.setattr("torch.npu.device_count", mock_device_count)
        monkeypatch.setattr("torch.npu.mem_get_info", mock_mem_get_info)
        monkeypatch.setattr("torch.npu.reset_peak_memory_stats", mock_reset_peak)
        monkeypatch.setattr("torch.npu.max_memory_allocated", mock_max_memory)

        # Test set_device
        test_device = torch.device("npu:0")
        NPUPlatform.set_device(test_device)
        mock_set_device.assert_called_once_with(test_device)

        # Test get_device_name
        result = NPUPlatform.get_device_name(0)
        assert result == "Ascend910"
        mock_get_device_name.assert_called_once_with(0)

        # Test device_count
        result = NPUPlatform.device_count()
        assert result == 8
        mock_device_count.assert_called_once()

        # Test mem_get_info
        result = NPUPlatform.mem_get_info()
        assert result == (1000, 2000)
        mock_mem_get_info.assert_called_once()

        # Test get_current_memory_usage
        result = NPUPlatform.get_current_memory_usage(test_device)
        assert result == 500.0
        mock_reset_peak.assert_called_once_with(test_device)
        mock_max_memory.assert_called_once_with(test_device)

    def test_inference_mode(self):
        """Test inference_mode method.
        
        Verifies that inference_mode returns a context manager
        similar to torch.no_grad().
        """
        result = NPUPlatform.inference_mode()
        assert isinstance(result, torch.no_grad().__class__)

    def test_import_kernels(self, monkeypatch):
        """Test import_kernels method.
        
        Verifies that import_kernels calls patch_compile_decorators
        and register_connectors.
        """
        patch_decorators_called = {"called": False}
        register_connectors_called = {"called": False}

        def mock_patch_decorators():
            patch_decorators_called["called"] = True

        def mock_register_connectors():
            register_connectors_called["called"] = True

        monkeypatch.setattr(
            "omni_npu.compilation.decorators.patch_compile_decorators",
            mock_patch_decorators,
        )
        monkeypatch.setattr(
            "omni_npu.connector.register_connectors",
            mock_register_connectors,
        )

        NPUPlatform.import_kernels()
        assert patch_decorators_called["called"] is True
        assert register_connectors_called["called"] is True

    def test_pre_register_and_update(self, monkeypatch):
        """Test pre_register_and_update method (covers lines 111-112).
        
        This method mainly imports modules, we just need to ensure
        it doesn't raise exceptions and imports are successful.
        """
        # Verify imports don't raise exceptions
        NPUPlatform.pre_register_and_update()
        # The method should complete without errors

    def test_get_punica_wrapper(self):
        """Test get_punica_wrapper method.
        
        Verifies that the correct Punica wrapper class name is returned.
        """
        result = NPUPlatform.get_punica_wrapper()
        assert result == "vllm.lora.punica_wrapper.punica_cpu.PunicaWrapperCPU"

    def test_get_device_communicator_cls(self):
        """Test get_device_communicator_cls method.
        
        Verifies that the correct device communicator class name is returned.
        """
        result = NPUPlatform.get_device_communicator_cls()
        assert result == "omni_npu.distributed.communicator.NPUCommunicator"

    def test_get_attn_backend_cls(self, monkeypatch):
        """Test get_attn_backend_cls method.
        
        Verifies that the correct attention backend class is returned
        based on use_mla and use_sparse flags, with and without VLLM_PLUGINS.
        """
        # Test use_mla=True, use_sparse=True (without VLLM_PLUGINS, covers line 161)
        monkeypatch.delenv("VLLM_PLUGINS", raising=False)
        result = NPUPlatform.get_attn_backend_cls(
            selected_backend="test",
            head_size=64,
            dtype=torch.float16,
            kv_cache_dtype="float16",
            block_size=16,
            use_mla=True,
            has_sink=False,
            use_sparse=True,
        )
        assert result == "omni_npu.attention.backends.dsa.NPUDSABackend"

        # Test use_mla=True, use_sparse=False
        result = NPUPlatform.get_attn_backend_cls(
            selected_backend="test",
            head_size=64,
            dtype=torch.float16,
            kv_cache_dtype="float16",
            block_size=16,
            use_mla=True,
            has_sink=False,
            use_sparse=False,
        )
        assert result == "omni_npu.attention.backends.mla.NPUMLABackend"

        # Test use_mla=False
        result = NPUPlatform.get_attn_backend_cls(
            selected_backend="test",
            head_size=64,
            dtype=torch.float16,
            kv_cache_dtype="float16",
            block_size=16,
            use_mla=False,
            has_sink=False,
            use_sparse=False,
        )
        assert result == "omni_npu.attention.backends.attention.NPUAttentionBackend"
        
        # Test with VLLM_PLUGINS containing "omni_custom_models" (covers lines 150-157)
        monkeypatch.setenv("VLLM_PLUGINS", "omni_custom_models")
        result = NPUPlatform.get_attn_backend_cls(
            selected_backend="test",
            head_size=64,
            dtype=torch.float16,
            kv_cache_dtype="float16",
            block_size=16,
            use_mla=True,
            has_sink=False,
            use_sparse=True,
        )
        assert result == "omni_npu.attention.backends.dsa.NPUDSABackend"
        
        result = NPUPlatform.get_attn_backend_cls(
            selected_backend="test",
            head_size=64,
            dtype=torch.float16,
            kv_cache_dtype="float16",
            block_size=16,
            use_mla=True,
            has_sink=False,
            use_sparse=False,
        )
        assert result == "omni_npu.v1.attention.backends.mla.NPUMLABackend"
        
        result = NPUPlatform.get_attn_backend_cls(
            selected_backend="test",
            head_size=64,
            dtype=torch.float16,
            kv_cache_dtype="float16",
            block_size=16,
            use_mla=False,
            has_sink=False,
            use_sparse=False,
        )
        assert result == "omni_npu.attention.backends.attention.NPUAttentionBackend"

    def test_simple_compile_backend(self):
        """Test simple_compile_backend property.
        
        Verifies that simple_compile_backend returns "eager".
        """
        platform = NPUPlatform()
        assert platform.simple_compile_backend == "eager"

    def test_support_static_graph_mode(self):
        """Test support_static_graph_mode method.
        
        Verifies that static graph mode is supported.
        """
        result = NPUPlatform.support_static_graph_mode()
        assert result is True

    def test_get_static_graph_wrapper_cls(self):
        """Test get_static_graph_wrapper_cls method.
        
        Verifies that the correct static graph wrapper class name is returned.
        """
        result = NPUPlatform.get_static_graph_wrapper_cls()
        assert result == "omni_npu.compilation.acl_graph.ACLGraphWrapper"

    def test_check_and_update_config(self, monkeypatch):
        """Test check_and_update_config method.
        
        Verifies that configuration is properly updated with NPU-specific
        settings including worker class, block size, and compilation config.
        """
        vllm_config = self.vllm_cfg

        # Mock ConfigUpdater.update_vllm_config
        update_called = {"called": False}

        def mock_update_vllm_config(config):
            update_called["called"] = True
            # Set necessary attributes
            config.npu_compilation_config = SimpleNamespace(
                use_gegraph=False,
                build_from_cli=lambda *args: None,
            )
            config.parallel_config = SimpleNamespace(worker_cls=None)
            config.cache_config = SimpleNamespace(block_size=None)
            config.model_config = SimpleNamespace(use_mla=False)
            config.scheduler_config = SimpleNamespace(
                enable_chunked_prefill=True,
                chunked_prefill_enabled=True,
            )
            config.compilation_config = SimpleNamespace(
                pass_config=SimpleNamespace(
                    fuse_norm_quant=None,
                    fuse_act_quant=None,
                    fuse_attn_quant=None,
                )
            )

        monkeypatch.setattr(
            "omni_npu.platform.ConfigUpdater.update_vllm_config",
            mock_update_vllm_config,
        )

        NPUPlatform.check_and_update_config(vllm_config)

        assert update_called["called"] is True
        assert vllm_config.parallel_config.worker_cls == "omni_npu.worker.npu_worker.NPUWorker"
        assert vllm_config.cache_config.block_size == 128
        assert vllm_config.compilation_config.pass_config.fuse_norm_quant is False
        assert vllm_config.compilation_config.pass_config.fuse_act_quant is False
        assert vllm_config.compilation_config.pass_config.fuse_attn_quant is False

    def test_check_and_update_config_with_mla(self, monkeypatch):
        """Test configuration update when MLA is enabled.
        
        Verifies that when MLA is enabled, chunked prefill is disabled
        in the scheduler configuration.
        """
        vllm_config = self.vllm_cfg

        def mock_update_vllm_config(config):
            config.npu_compilation_config = SimpleNamespace(
                use_gegraph=False,
                build_from_cli=lambda *args: None,
            )
            config.parallel_config = SimpleNamespace(worker_cls=None)
            config.cache_config = SimpleNamespace(block_size=128)
            config.model_config = SimpleNamespace(use_mla=True)
            config.scheduler_config = SimpleNamespace(
                enable_chunked_prefill=True,
                chunked_prefill_enabled=True,
            )
            config.compilation_config = SimpleNamespace(
                pass_config=SimpleNamespace(
                    fuse_norm_quant=None,
                    fuse_act_quant=None,
                    fuse_attn_quant=None,
                )
            )

        monkeypatch.setattr(
            "omni_npu.platform.ConfigUpdater.update_vllm_config",
            mock_update_vllm_config,
        )

        NPUPlatform.check_and_update_config(vllm_config)

        assert vllm_config.cache_config.block_size is 128
        assert vllm_config.compilation_config.pass_config.fuse_norm_quant is False


class TestConfigUpdater:
    def __init__(self):
        self.vllm_cfg = create_vllm_config()

    def test_update_vllm_config(self, monkeypatch):
        """Test update_vllm_config method.
        
        Verifies that vLLM configuration is updated with NPU compilation
        config, handling both cases with and without additional_config.
        """
        vllm_config = self.vllm_cfg

        # Mock NPUCompilationConfig
        mock_compilation_config = SimpleNamespace(
            use_gegraph=False,
            build_from_cli=MagicMock(),
        )

        def mock_npu_compilation_config():
            return mock_compilation_config

        monkeypatch.setattr(
            "omni_npu.compilation.ge_compile_config.NPUCompilationConfig",
            mock_npu_compilation_config,
        )

        # Mock supports_dynamo
        monkeypatch.setattr(
            "vllm.utils.torch_utils.supports_dynamo",
            lambda: True,
        )

        # Test case: no additional_config
        vllm_config.additional_config = None
        ConfigUpdater.update_vllm_config(vllm_config)
        assert vllm_config.npu_compilation_config is mock_compilation_config

        # Test case: with additional_config (covers lines 32-34)
        vllm_config.additional_config = {
            "graph_model_compile_config": {"key": "value"}
        }
        ConfigUpdater.update_vllm_config(vllm_config)
        # Verify build_from_cli was called with correct arguments
        mock_compilation_config.build_from_cli.assert_called_once()
        call_args = mock_compilation_config.build_from_cli.call_args
        assert call_args[0][0] == {"key": "value"}  # graph_model_compile_config
        assert call_args[0][1] == vllm_config  # vllm_config

    def test_update_vllm_config_with_env_var(self, monkeypatch):
        """Test the effect of TORCH_COMPILE_GE environment variable.
        
        Verifies that the TORCH_COMPILE_GE environment variable correctly
        sets the use_gegraph flag in the compilation config.
        """
        vllm_config = self.vllm_cfg

        mock_compilation_config = SimpleNamespace(
            use_gegraph=False,
            build_from_cli=MagicMock(),
        )

        def mock_npu_compilation_config():
            return mock_compilation_config

        monkeypatch.setattr(
            "omni_npu.compilation.ge_compile_config.NPUCompilationConfig",
            mock_npu_compilation_config,
        )
        monkeypatch.setattr(
            "vllm.utils.torch_utils.supports_dynamo",
            lambda: True,
        )

        # Test TORCH_COMPILE_GE=True
        monkeypatch.setenv("TORCH_COMPILE_GE", "true")
        vllm_config.additional_config = None
        ConfigUpdater.update_vllm_config(vllm_config)
        assert vllm_config.npu_compilation_config.use_gegraph is True

        # Test TORCH_COMPILE_GE=False
        monkeypatch.setenv("TORCH_COMPILE_GE", "false")
        ConfigUpdater.update_vllm_config(vllm_config)
        assert vllm_config.npu_compilation_config.use_gegraph is False

    def test_update_vllm_config_no_dynamo(self, monkeypatch):
        """Test case when dynamo is not supported (covers lines 41-42).
        
        Verifies that when dynamo is not supported, use_gegraph
        is set to False regardless of other settings, and warning is logged.
        """
        vllm_config = self.vllm_cfg

        mock_compilation_config = SimpleNamespace(
            use_gegraph=True,
            build_from_cli=MagicMock(),
        )

        def mock_npu_compilation_config():
            return mock_compilation_config

        monkeypatch.setattr(
            "omni_npu.compilation.ge_compile_config.NPUCompilationConfig",
            mock_npu_compilation_config,
        )
        monkeypatch.setattr(
            "vllm.utils.torch_utils.supports_dynamo",
            lambda: False,
        )
        
        # Mock logger.warning to verify it's called (covers line 42)
        warning_called = {"called": False}
        def mock_warning(msg):
            warning_called["called"] = True
        
        monkeypatch.setattr("omni_npu.platform.logger.warning", mock_warning)

        vllm_config.additional_config = None
        ConfigUpdater.update_vllm_config(vllm_config)
        assert vllm_config.npu_compilation_config.use_gegraph is False
        assert warning_called["called"] is True

