{
    "model_parallel_config": {
        "layer_parallel_config": {
            "self_attn.o_proj": {
                "tp_size_or_ranks": 1,
                "x_transform": {
                    "type": "None"
                },
                "y_transform": {
                    "type": "AllReduce",
                    "tp_size_or_ranks": 1
                }
            },
            "mlp.down_proj": {
                "tp_size_or_ranks": 4,
                "x_transform": {
                    "type": "None"
                },
                "y_transform": {
                    "type": "AllReduce",
                    "tp_size_or_ranks": 4
                }
            },
            "mlp.gate_up_proj": {
                "tp_size_or_ranks": 4,
                "x_transform": {
                    "type": "None"
                },
                "y_transform": {
                    "type": "AllReduce",
                    "tp_size_or_ranks": 4
                }
            }
        }
    },
    "operator_optimization_config": {
        "enable_kv_rmsnorm_rope_cache": true,
        "prefill_moe_all_to_all": true,
        "moe_multi_stream_tune": true,
        "best_ep": false,
        "merge_qkv": false,
        "gmm_nz": true,
        "decode_moe_dispatch_combine": true,
        "enable_super_kernel": false,
        "enable_mlaprolog": false,
        "cast_w2_scale_f32": true,
        "control_accept_rate": -1,
        "enable_prefetch": true,
        "expert_gate_up_prefetch": 50,
        "expert_down_prefetch": 0,
        "attn_prefetch": 96
    }
}
