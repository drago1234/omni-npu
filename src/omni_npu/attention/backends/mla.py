"""
Minimal, self-contained NPU MLA attention backend for omni_npu.

This implementation currently delegates to the standard NPU attention
backend to remain fully self-contained and avoid external dependencies.
It satisfies vLLM's backend interface so the platform selector can
import and use it. We can iterate later with true MLA specialization.
"""
from dataclasses import dataclass
from typing import ClassVar, Optional, Tuple
import math

import torch
import torch_npu

from vllm.attention.backends.abstract import AttentionLayer, AttentionType
from vllm.forward_context import get_forward_context
from vllm.config import VllmConfig, get_current_vllm_config
from vllm.logger import init_logger
from vllm.v1.attention.backends.mla.common import (
    MLACommonBackend,
    MLACommonDecodeMetadata,
    MLACommonMetadata,
    MLACommonMetadataBuilder,
    MLACommonBaseImpl,
    QueryLenSupport,
)
from vllm.distributed.parallel_state import get_tp_group
from omni_npu.connector.utils import TP_Convertor
from vllm.v1.attention.backends.utils import AttentionCGSupport, CommonAttentionMetadata
from vllm.v1.kv_cache_interface import AttentionSpec
from vllm.platforms import current_platform
from omni_npu.attention import ops


logger = init_logger(__name__)


class NPUMLABackend(MLACommonBackend):
    @staticmethod
    def get_name() -> str:
        return "NPUMLA"

    @staticmethod
    def get_metadata_cls() -> type["NPUMLAMetadata"]:
        return NPUMLAMetadata

    @staticmethod
    def get_builder_cls() -> type["NPUMLAMetadataBuilder"]:
        return NPUMLAMetadataBuilder

    @staticmethod
    def get_impl_cls() -> type["NPUMLAImpl"]:
        return NPUMLAImpl

    @staticmethod
    def reshape_kv_cache(
        raw_tensor: torch.Tensor,
        num_blocks: int,
        block_size: int,
        num_kv_heads: int,
        head_size: int,
        dtype: torch.dtype = torch.bfloat16,
    ) -> Tuple[torch.Tensor, ...]:
        raw_tensor = raw_tensor.view(dtype=dtype)
        shapes = [(num_blocks, block_size, 512), (num_blocks, block_size, 64)]
        sizes = [math.prod(shape) for shape in shapes]
        if raw_tensor.numel() != sum(sizes):
            raise RuntimeError(f"Raw tensor has {raw_tensor.numel()} elements, while"
                               f" the expected sizes for KV cache are {sizes}.")
        tensors = torch.split(raw_tensor, sizes)
        return tuple(t.view(shape) for t, shape in zip(tensors, shapes))


@dataclass
class NPUMLADecodeMetadata(MLACommonDecodeMetadata):
    query_cumlens: torch.Tensor
    mc2_mask: torch.Tensor = None

@dataclass
class NPUMLAMetadata(MLACommonMetadata[NPUMLADecodeMetadata]):
    pass


class NPUMLAMetadataBuilder(MLACommonMetadataBuilder[NPUMLAMetadata]):
    _cudagraph_support: ClassVar[AttentionCGSupport] = AttentionCGSupport.ALWAYS
    query_len_support: ClassVar[QueryLenSupport] = QueryLenSupport.VARLEN

    def __init__(
        self,
        kv_cache_spec: AttentionSpec,
        layer_names: list[str],
        vllm_config: VllmConfig,
        device: torch.device,
    ):
        super().__init__(
            kv_cache_spec, layer_names, vllm_config, device, NPUMLAMetadata
        )

        if self._use_fi_prefill:
            raise ValueError("Flashinfer should not be enabled.")
        if self._use_cudnn_prefill:
            raise ValueError("CUDNN should not be enabled.")
        if self.dcp_local_block_size != 1:
            raise ValueError("DCP only support cp_kv_cache_interleave_size == 1.")
        if self.aot_schedule:
            raise ValueError("AOT schedule should be enabled.")

        if self.compilation_config is not None:
            self.reorder_batch_threshold = max(self.compilation_config.max_cudagraph_capture_size, self.reorder_batch_threshold)
        # FIXME (zhao): since current the max length of input of mc2_mask only support 256, so we clamp it to 256
        max_decode_tokens = min(256, self.vllm_config.scheduler_config.max_num_seqs * self.reorder_batch_threshold)
        self.mc2_mask = torch.zeros(max_decode_tokens, dtype=torch.bool, device=current_platform.device_type)

    def _build_decode(
        self,
        block_table_tensor: torch.Tensor,
        seq_lens_cpu: torch.Tensor,
        seq_lens_device: torch.Tensor,
        query_start_loc_cpu: torch.Tensor,
        query_start_loc_device: torch.Tensor,
        num_decode_tokens: int,
        dcp_tot_seq_lens_device: torch.Tensor | None,
    ) -> NPUMLADecodeMetadata:
        return NPUMLADecodeMetadata(
            block_table=block_table_tensor,
            seq_lens=seq_lens_cpu.tolist(),
            query_cumlens=query_start_loc_cpu[1:].tolist(),
            dcp_tot_seq_lens=dcp_tot_seq_lens_device
        )

    def build(
        self,
        common_prefix_len: int,
        common_attn_metadata: CommonAttentionMetadata,
        fast_build: bool = False,
    ) -> NPUMLAMetadata:
        metadata = super().build(common_prefix_len, common_attn_metadata, fast_build)
        if metadata.decode is not None and self.vllm_config.kv_transfer_config is not None:
            # for pd-mixed, TP is used, no need to use mc2_mask
            metadata.decode.mc2_mask = self.generate_activate_mask(common_attn_metadata.num_actual_tokens)
        if metadata.prefill is not None:
            metadata.prefill.query_start_loc_list = metadata.prefill.query_start_loc.tolist()
        if self.dcp_world_size > 1:
            self.prepare_dcp_slots(metadata)
            self.prepare_dcp_ag_reorg(metadata)
            # for D node in PD-seperate(D-TP >= P-TP) mode
            TP_Convertor.do_scheduled_kv_reorg()
        return metadata

    def generate_activate_mask(self, actual_seqs_num):
        self.mc2_mask.fill_(False)
        self.mc2_mask[:actual_seqs_num].fill_(True)
        return self.mc2_mask

    # @staticmethod
    # def determine_chunked_prefill_workspace_size(_):
    #     return 512

    def prepare_dcp_slots(self, metadata):
        slots = metadata.slot_mapping
        cfg = {"dtype": torch.int32, "device": slots.device}
        kv_idx = torch.arange(slots.size(0), **cfg)[slots != -1]

        metadata.dcp_local_kv_idx = kv_idx
        metadata.dcp_local_slots = slots[kv_idx].to(**cfg)

    def prepare_dcp_ag_reorg(self, metadata, pg = 128):
        prefill_meta = metadata.prefill
        if prefill_meta is None:
            return
        chunk_ctx = prefill_meta.chunked_context
        if chunk_ctx is None:
            return

        starts = chunk_ctx.starts                    # npu.int32[chk, seq]   start_pos of dcp chunks
        cu_lens = chunk_ctx.padded_local_cu_seq_lens # npu.int32[chk, seq+1] token_num of dcp chunks
        g_cu_lens = chunk_ctx.cu_seq_lens            # npu.int32[chk, seq+1] token_num of chunks
        blk_table = prefill_meta.block_table         # npu.int32[seq, *]     kv_cache slot mapping

        chunk_ctx.dcp_local_idx = [ # kv gather idx for dcp_chunk[i]
            self.paged_index(cu_lens_i, starts_i, blk_table, pg)
            for cu_lens_i, starts_i in zip(cu_lens, starts)
        ]
        chunk_ctx.dcp_reorg_order = [ # reorg after all-gather
            self.reorg_index(cu_lens_i, g_cu_lens_i, self.dcp_world_size)
            for cu_lens_i, g_cu_lens_i in zip(cu_lens, g_cu_lens)
        ]

    @staticmethod
    def paged_index(
        cu_lens:torch.Tensor, # int32[seq+1]
        starts:torch.Tensor,  # int32[seq]
        table:torch.Tensor,   # int32[seq, *]
        pg:int = 128,
    ):
        assert cu_lens.dim() == 1
        assert starts.dim() == 1
        assert table.dim() == 2
        assert starts.size(0) + 1 == cu_lens.size(0)
        assert starts.size(0) == table.size(0)
        seq_lens = cu_lens.diff()
        cfg = {"dtype":torch.int32, "device":starts.device}
        serial = torch.arange(cu_lens[-1], **cfg)
        token_seq = torch.arange(seq_lens.size(0), **cfg).repeat_interleave(seq_lens, dim=0)
        token_pos = serial + (starts - cu_lens[:-1]).repeat_interleave(seq_lens, dim=0)
        return table[token_seq, token_pos // pg] * pg + token_pos % pg

    @staticmethod
    def reorg_index(
        local_cu_lens:torch.Tensor,  # int32[seq+1]
        global_cu_lens:torch.Tensor, # int32[seq+1]
        dcp:int = 16,
    ):
        assert local_cu_lens.dim() == 1
        assert global_cu_lens.dim() == 1
        assert local_cu_lens.size(0) == global_cu_lens.size(0)
        global_lens = global_cu_lens.diff()
        serial = torch.arange(global_cu_lens[-1], dtype=torch.int32, device=global_lens.device)
        token_pos = serial - global_cu_lens[:-1].repeat_interleave(global_lens, dim=0)
        offset = local_cu_lens[:-1].repeat_interleave(global_lens, dim=0)
        return offset + (token_pos // dcp) + (token_pos % dcp * local_cu_lens[-1])


class NPUMLAImpl(MLACommonBaseImpl[NPUMLAMetadata]):
    can_return_lse_for_decode: bool = True
    SHARE_MASK_TRIL_SPARSE = None

    def __init__(
        self,
        num_heads: int,
        head_size: int,
        scale: float,
        num_kv_heads: int,
        alibi_slopes: Optional[list[float]],
        sliding_window: Optional[int],
        kv_cache_dtype: str,
        logits_soft_cap: Optional[float],
        attn_type: str,
        kv_sharing_target_layer_name: Optional[str],
        # MLA Specific Arguments
        **mla_args,
    ) -> None:
        super().__init__(
            num_heads,
            head_size,
            scale,
            num_kv_heads,
            alibi_slopes,
            sliding_window,
            kv_cache_dtype,
            logits_soft_cap,
            attn_type,
            kv_sharing_target_layer_name,
            **mla_args,
        )

        self.chunked_prefill_workspace_size = (
            MLACommonMetadataBuilder.determine_chunked_prefill_workspace_size(
                get_current_vllm_config()
            )
        )

        self.sink_k_pe = None
        self.sink_compressed_kv = None
        self.sink_len = 0

        unsupported_features = [alibi_slopes, sliding_window, logits_soft_cap]
        if any(unsupported_features):
            raise NotImplementedError(
                "NPUMLAImpl does not support one of the following: "
                "alibi_slopes, sliding_window, logits_soft_cap"
            )

        if attn_type != AttentionType.DECODER:
            raise NotImplementedError(
                "Encoder self-attention and "
                "encoder/decoder cross-attention "
                "are not implemented for "
                "NPUMLAImpl"
            )

        if NPUMLAImpl.SHARE_MASK_TRIL_SPARSE is None:
            NPUMLAImpl.SHARE_MASK_TRIL_SPARSE = ~torch.tril(
                torch.ones((2048, 2048), dtype=torch.bool, device="npu")
            )
            NPUMLAImpl.DECORE_ATTN_MASK = NPUMLAImpl.SHARE_MASK_TRIL_SPARSE.to(torch.uint8)

    def update_sink_kv(self, sink_k_pe: torch.Tensor, sink_compressed_kv: torch.Tensor) -> None:
        self.sink_k_pe = sink_k_pe.unsqueeze(1)
        self.sink_compressed_kv = sink_compressed_kv
        self.sink_len = sink_compressed_kv.shape[0]

    def _v_up_proj(self, x: torch.Tensor):
        x = x.view(self.num_heads, -1, self.kv_lora_rank)

        # Multiply (N, B, L) x (N, L, V) -> (N, B, V)
        out2 = torch.bmm(x, self.W_UV)
        out_new = out2.transpose(0, 1).contiguous().view(-1, self.num_heads * self.v_head_dim)
        return out_new

    def _compute_prefill_context_dcp(
        self,
        q_nope: torch.Tensor,
        q_pe: torch.Tensor,
        kv_cache: Tuple[torch.Tensor, torch.Tensor],
        attn_meta: MLACommonMetadata,
    ):
        prefill_meta = attn_meta.prefill
        assert prefill_meta is not None
        chunk_ctx = prefill_meta.chunked_context
        assert chunk_ctx is not None

        for i, toks in enumerate(chunk_ctx.seq_tot): # for each chunk
            def kv_ag_reorg(cache):
                cache = cache.flatten(end_dim=-2)                # [*, 128, D] -> [*, D]
                cache = cache[chunk_ctx.dcp_local_idx[i]]        # prepare local
                cache = get_tp_group().all_gather(cache, dim=0) # all_gather
                return cache[chunk_ctx.dcp_reorg_order[i]]       # reorg

            kv_c_normed, k_pe = (kv_ag_reorg(it) for it in kv_cache)

            kv_nope = self.kv_b_proj(kv_c_normed)[0]
            kv_nope = kv_nope.view(-1, self.num_heads, self.qk_nope_head_dim + self.v_head_dim)
            k_nope, v = kv_nope.split([self.qk_nope_head_dim, self.v_head_dim], dim=-1)
            k_pe = k_pe.view(-1, 1, self.qk_rope_head_dim).repeat(1, self.num_heads, 1)
            cu_q_lens = prefill_meta.query_start_loc_list[1:]
            cu_kv_lens = chunk_ctx.cu_seq_lens[i, 1:]

            suffix_out, suffix_lse = torch_npu.npu_fused_infer_attention_score(
                q_nope, k_nope, v,
                query_rope=q_pe,
                key_rope=k_pe,
                num_heads=self.num_heads,
                num_key_value_heads=self.num_heads,
                input_layout="TND",
                actual_seq_lengths=cu_q_lens,
                actual_seq_lengths_kv=cu_kv_lens,
                scale=self.scale,
                softmax_lse_flag=True,
            )

            if i == 0:
                out = suffix_out
                lse = suffix_lse
            else:
                prefix_out=out
                prefix_lse=lse
                out = torch.empty_like(prefix_out, dtype=torch.float32)
                lse = torch.empty_like(prefix_lse, dtype=torch.float32)
                ops.merge_attn_states(
                    output=out,
                    output_lse=lse,
                    prefix_output=prefix_out,
                    prefix_lse=prefix_lse,
                    suffix_output=suffix_out,
                    suffix_lse=suffix_lse,
                )

        return out, lse

    def _compute_prefill_context(
        self,
        q_nope: torch.Tensor,
        q_pe: torch.Tensor,
        kv_c_cache: torch.Tensor,
        k_pe_cache: torch.Tensor,
        attn_metadata: MLACommonMetadata,
        k_scale: torch.Tensor,
    ):
        assert attn_metadata.prefill is not None
        prefill_metadata = attn_metadata.prefill
        assert prefill_metadata.chunked_context is not None

        output = None
        iters = len(prefill_metadata.chunked_context.seq_tot)
        workspace = prefill_metadata.chunked_context.workspace

        for i in range(iters):
            toks = prefill_metadata.chunked_context.seq_tot[i]
            ops.gather_and_maybe_dequant_cache(
                src_cache=(kv_c_cache, k_pe_cache),
                dst=workspace,
                block_table=prefill_metadata.block_table,
                cu_seq_lens=prefill_metadata.chunked_context.cu_seq_lens[i],
                batch_size=attn_metadata.num_prefills,
                kv_cache_dtype=self.kv_cache_dtype,
                scale=k_scale,
                seq_starts=prefill_metadata.chunked_context.starts[i],
            )

            kv_c_normed = workspace[:toks][..., : self.kv_lora_rank]
            k_pe = workspace[:toks][..., self.kv_lora_rank :].unsqueeze(1)

            kv_nope = self.kv_b_proj(kv_c_normed)[0].view(
                -1, self.num_heads, self.qk_nope_head_dim + self.v_head_dim
            )
            k_nope, v = kv_nope.split([self.qk_nope_head_dim, self.v_head_dim], dim=-1)

            attn_output, attn_softmax_lse = torch.ops.npu.npu_fused_infer_attention_score(
                q_nope,
                k_nope,
                v,
                query_rope=q_pe,
                key_rope=k_pe.view(-1, 1, self.qk_rope_head_dim).repeat(1, self.num_heads, 1),
                num_heads=self.num_heads,
                num_key_value_heads=self.num_heads,
                input_layout="TND",
                atten_mask=None,
                sparse_mode=0,  # for prefix, no mask on attention matrix
                actual_seq_lengths=prefill_metadata.query_start_loc_list[1:],
                actual_seq_lengths_kv=prefill_metadata.chunked_context.cu_seq_lens[i, 1:],
                scale=self.scale,
                next_tokens=0,
                softmax_lse_flag=True,
            )

            if output is None:
                output = attn_output
                output_lse = attn_softmax_lse
            else:
                output_tmp = torch.empty_like(output)
                output_lse_tmp = torch.empty_like(output_lse)
                ops.merge_attn_states(
                    output=output_tmp,
                    output_lse=output_lse_tmp,
                    prefix_output=output,
                    prefix_lse=output_lse,
                    suffix_output=attn_output,
                    suffix_lse=attn_softmax_lse,
                )
                output = output_tmp
                output_lse = output_lse_tmp

        return output, output_lse

    def _forward_prefill(
        self,
        q: torch.Tensor,
        kv_c_normed: torch.Tensor,
        k_pe: torch.Tensor,
        kv_cache: Tuple[torch.Tensor, torch.Tensor],
        attn_metadata: NPUMLAMetadata,
        k_scale: torch.Tensor,
    ) -> torch.Tensor:
        assert attn_metadata.prefill is not None

        # When sink tokens are used, we need to insert cached sink tokens at the beginning of each sequence
        if self.sink_len > 0:
            k_pe = self._insert_tensor_by_start_loc(
                k_pe,
                self.sink_k_pe,
                attn_metadata.prefill.query_start_loc,
            )
            kv_c_normed = self._insert_tensor_by_start_loc(
                kv_c_normed,
                self.sink_compressed_kv,
                attn_metadata.prefill.query_start_loc,
            )

        kv_nope = self.kv_b_proj(kv_c_normed)[0].view(
            -1, self.num_heads, self.qk_nope_head_dim + self.v_head_dim
        )

        has_context = attn_metadata.prefill.chunked_context is not None
        q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)
        k_nope, v = kv_nope.split([self.qk_nope_head_dim, self.v_head_dim], dim=-1)
        tnd_cumlens = attn_metadata.prefill.query_start_loc_list[1:]

        # When sink tokens are used, the actual sequence lengths for key and value are different.
        num_prefills = len(tnd_cumlens)
        sink_len_offset = [self.sink_len * (i + 1) for i in range(num_prefills)]
        kv_cumlens = [x + y for x, y in zip(tnd_cumlens, sink_len_offset)]

        output, output_lse = torch.ops.npu.npu_fused_infer_attention_score(
            q_nope,
            k_nope,
            v,
            query_rope=q_pe,
            key_rope=k_pe.view(-1, 1, self.qk_rope_head_dim).repeat(1, self.num_heads, 1),
            num_heads=self.num_heads,
            num_key_value_heads=self.num_heads,
            input_layout="TND",
            atten_mask=NPUMLAImpl.SHARE_MASK_TRIL_SPARSE,
            sparse_mode=3,
            actual_seq_lengths=tnd_cumlens,
            actual_seq_lengths_kv=kv_cumlens,
            scale=self.scale,
            next_tokens=0,
            softmax_lse_flag=has_context,
        )

        if has_context:
            if self.dcp_world_size > 1:
                context_output, context_lse = self._compute_prefill_context_dcp(
                    q_nope, q_pe, kv_cache, attn_metadata,
                ) # DCP not support scaled kvcache now
            else:
                context_output, context_lse = self._compute_prefill_context(
                    q_nope=q_nope,
                    q_pe=q_pe,
                    kv_c_cache=kv_cache[0],
                    k_pe_cache=kv_cache[1],
                    attn_metadata=attn_metadata,
                    k_scale=k_scale,
                )
            merged_output = torch.empty_like(output, dtype=torch.float32)
            ops.merge_attn_states(
                output=merged_output,
                prefix_output=context_output,
                prefix_lse=context_lse,
                suffix_output=output,
                suffix_lse=output_lse,
            )
            output = merged_output

        return output.to(torch.bfloat16).flatten(start_dim=-2)

    def _forward_decode_dcp(
        self,
        ql_nope: torch.Tensor,                        # [bs, 8, 512]
        q_pe: torch.Tensor,                           # [bs, 8, 512]
        kv_cache: Tuple[torch.Tensor, torch.Tensor],  # [*, pg, 512], [*, pg, 64]
        attn_meta: NPUMLAMetadata,
        layer: AttentionLayer,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:
        decode_meta = attn_meta.decode
        assert decode_meta is not None

        blk_table = decode_meta.block_table
        cu_q_lens = decode_meta.query_cumlens
        cu_kv_lens = decode_meta.seq_lens

        tp_group = get_tp_group().device_group
        tp_size = get_tp_group().world_size
        D = self.kv_lora_rank        # only support D=512
        N = self.num_heads * tp_size # only support head=128

        def gather_head(q, N, D):
            assert q.dim() == 3
            q = q.transpose(0, 1).flatten() # TND -> NTD
            q = get_tp_group().all_gather(q)
            return q.view(N, -1, D).transpose(0, 1) # NTD -> TND

        # DCP does not yet support graph mode or sink attn
        num_tokens = decode_meta.query_cumlens[-1]
        ql_nope = ql_nope[:num_tokens]
        q_pe = q_pe[:num_tokens]

        full_q_nope = gather_head(ql_nope, N, D) # TND
        full_q_rope = gather_head(q_pe, N, 64)   # TND

        out, lse = torch.ops.npu.npu_fused_infer_attention_score(
            full_q_nope,            # [T, N, D]
            kv_cache[0],            # [*, pg, D]
            kv_cache[0],            # [*, pg, D]
            query_rope=full_q_rope, # [T, N, 64]
            key_rope=kv_cache[1],   # [*, pg, 64]
            num_heads=N,
            num_key_value_heads=1,
            input_layout="TND_NTD",
            scale=self.scale,
            sparse_mode=3,
            atten_mask=NPUMLAImpl.DECORE_ATTN_MASK,
            block_size=128,
            block_table=blk_table,
            actual_seq_lengths=cu_q_lens,
            actual_seq_lengths_kv=cu_kv_lens,
            softmax_lse_flag=True,
        ) # -> out[N, T, D], lse[T, N, 1]

        cp_out = out.view(N, -1)                 # bf16[N, TD]
        cp_lse = lse.view(-1, N).transpose(0, 1) # fp32[N, T]
        tp_out = torch.empty_like(cp_out)        # bf16[N, TD]
        tp_lse = torch.empty_like(cp_lse)        # fp32[N, T]

        torch.distributed.all_to_all_single(tp_out.flatten(), cp_out.flatten(), group=tp_group)
        torch.distributed.all_to_all_single(tp_lse.flatten(), cp_lse.flatten(), group=tp_group)

        sect = [self.num_heads] * tp_size # head split pattern

        # TODO: "npu_attention_update" does not yet support tp > 16
        if tp_size <= 16:
            merged, _ = torch_npu.npu_attention_update(
                lse=[it.flatten() for it in tp_lse.split(sect, dim=0)],
                local_out=[it.view(-1, D) for it in tp_out.float().split(sect, dim=0)],
                update_type=0,
            )
        else:
            merged, _ = ops.attention_update_torch(
                outs=[it.view(-1, D) for it in tp_out.float().split(sect, dim=0)],
                lses=[it.flatten() for it in tp_lse.split(sect, dim=0)],
            )
        return merged.to(torch.bfloat16).view(self.num_heads, -1, D) # NTD

    def _forward_decode(
        self,
        decode_ql_nope: torch.Tensor,
        decode_q_pe: torch.Tensor,
        kv_cache: Tuple[torch.Tensor, torch.Tensor],
        attn_metadata: NPUMLAMetadata,
        layer: AttentionLayer,
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:
        assert attn_metadata.decode is not None

        if self.sink_len > 0:
            query_heads = 1 << (self.num_heads - 1).bit_length()
            pad_len = query_heads - self.num_heads
            ql_nope_pad = decode_ql_nope.new_empty((decode_ql_nope.shape[0], pad_len, decode_ql_nope.shape[-1]))
            decode_ql_nope = torch.cat([decode_ql_nope, ql_nope_pad], dim=1)
            q_pe_pad = decode_q_pe.new_empty((decode_q_pe.shape[0], pad_len, decode_q_pe.shape[-1]))
            decode_q_pe = torch.cat([decode_q_pe, q_pe_pad], dim=1)
        else:
            query_heads = self.num_heads

        # output shape: (N, T, D)
        o = torch.ops.npu.npu_fused_infer_attention_score(
            decode_ql_nope, kv_cache[0], kv_cache[0],
            query_rope=decode_q_pe,
            key_rope=kv_cache[1],
            num_heads=query_heads,
            num_key_value_heads=1,
            input_layout="TND_NTD",
            scale=self.scale,
            antiquant_mode=0,
            antiquant_scale=None,
            block_table=attn_metadata.decode.block_table,
            block_size=128,
            actual_seq_lengths=attn_metadata.decode.query_cumlens,
            actual_seq_lengths_kv=attn_metadata.decode.seq_lens,
            atten_mask = NPUMLAImpl.DECORE_ATTN_MASK,
            sparse_mode = 3
        )[0]

        if self.sink_len > 0:
            o = o[:self.num_heads]

        return o

    def forward(
        self,
        layer: AttentionLayer,
        q: torch.Tensor,
        k_c_normed: torch.Tensor,  # key in unified attn
        k_pe: torch.Tensor,  # value in unified attn
        kv_cache: Tuple[torch.Tensor, torch.Tensor],
        attn_metadata: NPUMLAMetadata,
        output: Optional[torch.Tensor] = None,
        output_scale: Optional[torch.Tensor] = None,
        output_block_scale: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        assert output is not None, "Output tensor must be provided."

        if output_scale is not None or output_block_scale is not None:
            raise NotImplementedError(
                "fused output quantization is not yet supported for NPUMLAImpl"
            )

        if attn_metadata is None:
            # During the profile run try to simulate to worse case output size
            # for `self.kv_b_proj(kv_c_normed)` in `_compute_prefill_context`
            # since this can be large
            _ = torch.empty(
                (
                    self.chunked_prefill_workspace_size,
                    self.num_heads,
                    self.qk_nope_head_dim + self.v_head_dim,
                ),
                device=k_c_normed.device,
                dtype=k_c_normed.dtype,
            )

            # The zero fill is required when used with DP + EP
            # to ensure all ranks within a DP group compute the
            # same expert outputs.
            return output.fill_(0)

        num_actual_toks = attn_metadata.num_actual_tokens
        assert isinstance(kv_cache, (list, tuple)), f"{type(kv_cache)=}."
        assert len(kv_cache) == 2, f"{len(kv_cache)=}."
        k_nope, k_rope = kv_cache
        assert isinstance(k_nope, torch.Tensor) and isinstance(k_rope, torch.Tensor), \
            f"{type(k_nope)=}, {type(k_rope)=}."
        assert k_nope.numel() > 0 and k_rope.numel() > 0, \
            f"{k_nope.shape=}, {k_rope.shape=}"

        # Inputs and outputs may be padded for CUDA graphs
        output_padded = output
        # logger.debug(f"<<< {output.shape=}, {q.shape=}, {k_c_normed.shape=}, {k_pe.shape=}")
        assert (
            attn_metadata.num_decodes is not None
            and attn_metadata.num_prefills is not None
            and attn_metadata.num_decode_tokens is not None
        )

        has_decode = attn_metadata.num_decodes > 0
        has_prefill = attn_metadata.num_prefills > 0
        num_decode_tokens = attn_metadata.num_decode_tokens

        decode_q = q[:num_decode_tokens]

        def store_kv(cache, kv):
            if self.dcp_world_size == 1:
                slots = attn_metadata.slot_mapping
            else:
                # TODO: DCP not yet support graph mode
                slots = attn_metadata.dcp_local_slots
                kv = kv[attn_metadata.dcp_local_kv_idx]
            cache = cache.flatten(end_dim=-2)
            slots = slots.view(-1, 1)
            torch_npu.npu_scatter_nd_update_(cache, slots, kv)

        # write the latent and rope to kv cache
        store_kv(kv_cache[0], k_c_normed)
        store_kv(kv_cache[1], k_pe.squeeze(1))

        if has_prefill:
            output = output[:num_actual_toks, ...]
            q = q[:num_actual_toks, ...]
            k_c_normed = k_c_normed[:num_actual_toks, ...]
            k_pe = k_pe[:num_actual_toks, ...]
            prefill_q = q[num_decode_tokens:]
            prefill_k_pe = k_pe[num_decode_tokens:]
            prefill_k_c_normed = k_c_normed[num_decode_tokens:]
            output[num_decode_tokens:] = self._forward_prefill(
                prefill_q,
                prefill_k_c_normed,
                prefill_k_pe,
                kv_cache,
                attn_metadata,
                layer._k_scale,
            )

        if has_decode:
            assert attn_metadata.decode is not None
            decode_q_nope, decode_q_pe = decode_q.split(
                [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1
            )
            # Convert from (B, N, P) to (N, B, P)
            decode_q_nope = decode_q_nope.transpose(0, 1)
            N, B, P = decode_q_nope.shape
            _, _, L = self.W_UK_T.shape
            # Multiply (N, B, P) x (N, P, L) -> (N, B, L)
            decode_ql_nope = torch.bmm(decode_q_nope, self.W_UK_T)
            # Convert from (N, B, L) to (B, N, L)
            decode_ql_nope = decode_ql_nope.transpose(0, 1)

            # call decode attn
            if self.dcp_world_size == 1:
                attn_out = self._forward_decode(
                    decode_ql_nope, decode_q_pe, kv_cache, attn_metadata, layer
                )
            else:
                attn_out = self._forward_decode_dcp(
                    decode_ql_nope, decode_q_pe, kv_cache, attn_metadata, layer
                )

            # v_up projection
            out_proj = self._v_up_proj(attn_out)
            output[:out_proj.shape[0]].copy_(out_proj)
        return output_padded

    @staticmethod
    def _insert_tensor_by_start_loc(
        raw_tensor: torch.Tensor, insert_segment: torch.Tensor, start_loc: list[int]
    ) -> torch.Tensor:
        segment_len = insert_segment.shape[0]
        num_inserts = len(start_loc) - 1
        total_len = segment_len * num_inserts + raw_tensor.shape[0]
        offset = 0
        # allocate result tensor
        result = torch.empty(total_len, *raw_tensor.shape[1:], device=raw_tensor.device, dtype=raw_tensor.dtype)

        for i in range(num_inserts):
            # write insert segment to result
            result[offset:offset+segment_len] = insert_segment
            offset += segment_len
            # write raw tensor to result
            seg_len = start_loc[i + 1] - start_loc[i]
            result[offset:offset+seg_len] = raw_tensor[start_loc[i]:start_loc[i+1]]
            offset += seg_len

        return result

